{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae885fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 11661 to 12805\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   step            500 non-null    int64  \n",
      " 1   type            500 non-null    object \n",
      " 2   amount          500 non-null    float64\n",
      " 3   nameOrig        500 non-null    object \n",
      " 4   oldbalanceOrg   500 non-null    float64\n",
      " 5   newbalanceOrig  500 non-null    float64\n",
      " 6   nameDest        500 non-null    object \n",
      " 7   oldbalanceDest  500 non-null    float64\n",
      " 8   newbalanceDest  500 non-null    float64\n",
      " 9   isFraud         500 non-null    int64  \n",
      " 10  isFlaggedFraud  500 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 46.9+ KB\n",
      "None\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "Training data shape: (400, 10)\n",
      "Testing data shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Import Libraries\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# =========================\n",
    "# 2. Load Dataset\n",
    "# =========================\n",
    "df = pd.read_csv(\"balanced_fraud_dataset.csv\")\n",
    "df=df.sample(500)\n",
    "df.to_csv(\"data.csv\",index=False)\n",
    "\n",
    "# =========================\n",
    "# 3. Basic Inspection\n",
    "# =========================\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# =========================\n",
    "# 4. Drop Unnecessary Columns\n",
    "# (IDs that don't help model)\n",
    "# =========================\n",
    "df = df.drop([\"nameOrig\", \"nameDest\"], axis=1)\n",
    "\n",
    "# =========================\n",
    "# 5. Feature Engineering\n",
    "# =========================\n",
    "# Balance difference features\n",
    "df[\"orgBalanceDiff\"] = df[\"oldbalanceOrg\"] - df[\"newbalanceOrig\"]\n",
    "df[\"destBalanceDiff\"] = df[\"newbalanceDest\"] - df[\"oldbalanceDest\"]\n",
    "\n",
    "# =========================\n",
    "# 6. Encode Categorical Feature\n",
    "# =========================\n",
    "le = LabelEncoder()\n",
    "df[\"type\"] = le.fit_transform(df[\"type\"])\n",
    "\n",
    "# =========================\n",
    "# 7. Define Features and Target\n",
    "# =========================\n",
    "X = df.drop(\"isFraud\", axis=1)\n",
    "y = df[\"isFraud\"]\n",
    "\n",
    "# =========================\n",
    "# 8. Train-Test Split\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 9. Feature Scaling\n",
    "# =========================\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape)\n",
    "print(\"Testing data shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b82658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\archit\\mlops\\Fraud-Detection-System--End-to-end-ml-project-\\myvenv\\lib\\site-packages\\requests\\__init__.py:113: RequestsDependencyWarning: urllib3 (2.6.3) or chardet (6.0.0.post1)/charset_normalizer (3.4.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ArchitSaki\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as ArchitSaki\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ArchitSaki/Fraud-Detection-System--End-to-end-ml-project-\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"ArchitSaki/Fraud-Detection-System--End-to-end-ml-project-\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ArchitSaki/Fraud-Detection-System--End-to-end-ml-project- initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository ArchitSaki/Fraud-Detection-System--End-to-end-ml-project- initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/ArchitSaki/Fraud-Detection-System--End-to-end-ml-project-.mlflow')\n",
    "dagshub.init(repo_owner='ArchitSaki', repo_name='Fraud-Detection-System--End-to-end-ml-project-', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99135d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/24 16:11:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/24 16:11:18 WARNING mlflow.sklearn: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization. The recommended safe alternative is the 'skops' format. For more information, see: https://scikit-learn.org/stable/model_persistence.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "üèÉ View run RandomForest_GridSearch at: https://dagshub.com/ArchitSaki/Fraud-Detection-System--End-to-end-ml-project-.mlflow/#/experiments/2/runs/bbb8d6d70cd14afc9c3a2e834452da8e\n",
      "üß™ View experiment at: https://dagshub.com/ArchitSaki/Fraud-Detection-System--End-to-end-ml-project-.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"f1_weighted\"\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(\"Hyperparameter_Tuning\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_GridSearch\"):\n",
    "\n",
    "    # Train GridSearch\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Log best parameters\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(best_model, \"best_model\")\n",
    "\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d850de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
